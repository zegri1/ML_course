{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T17:02:55.965907Z",
     "start_time": "2024-12-21T17:02:55.954907Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score\n",
    ")\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Load and prepare the data\n",
    "iris = load_iris(as_frame=True)\n",
    "data = iris['data']\n",
    "data['target'] = iris['target']\n",
    "feature_names = iris['feature_names']\n",
    "target_names = iris['target_names']\n",
    "\n",
    "# Exploratory Data Analysis\n",
    "def plot_correlations(data):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(data.corr(), annot=True, cmap='coolwarm')\n",
    "    plt.title('Feature Correlation Matrix')\n",
    "    plt.show()\n",
    "\n",
    "def plot_pair_plots(data):\n",
    "    sns.pairplot(data, hue='target', diag_kind='hist')\n",
    "    plt.show()\n",
    "\n",
    "# Data Preprocessing\n",
    "X = data.drop('target', axis=1)\n",
    "y = data['target']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Model Training and Evaluation\n",
    "def train_and_evaluate_model(model, X_train, X_test, y_train, y_test, model_name):\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n{model_name} Results:\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'Confusion Matrix - {model_name}')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5)\n",
    "    print(f\"Cross-validation scores: {cv_scores}\")\n",
    "    print(f\"Average CV score: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})\")"
   ],
   "id": "e1a83f6359a07845",
   "execution_count": 14,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T17:02:56.428926Z",
     "start_time": "2024-12-21T17:02:56.419929Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Perform EDA\n",
    "print(\"Data Shape:\", data.shape)\n",
    "print(\"\\nData Summary:\")\n",
    "print(data.describe())"
   ],
   "id": "96bdd9478493d0f0",
   "execution_count": 15,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T17:02:56.888146Z",
     "start_time": "2024-12-21T17:02:56.880898Z"
    }
   },
   "cell_type": "code",
   "source": "data",
   "id": "dcb0851c80c909e9",
   "execution_count": 16,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T17:02:59.458179Z",
     "start_time": "2024-12-21T17:02:57.722225Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Plot correlations and pair plots\n",
    "plot_correlations(data)\n",
    "plot_pair_plots(data)"
   ],
   "id": "9d25791744487b5d",
   "execution_count": 17,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T17:03:02.644029Z",
     "start_time": "2024-12-21T17:02:59.459187Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize models\n",
    "models = {\n",
    "    'Support Vector Machine': SVC(kernel='rbf'),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'Multi-Layer Perceptron': MLPClassifier(\n",
    "        hidden_layer_sizes=(100, 50),\n",
    "        max_iter=1000,\n",
    "        random_state=42\n",
    "    ),\n",
    "    'Random Forest': RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=None,\n",
    "        random_state=42\n",
    "    )\n",
    "}\n",
    "\n",
    "# Dictionary to store model performances\n",
    "model_performances = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    # Train and evaluate each model\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Get predictions\n",
    "    y_train_pred = model.predict(X_train_scaled)\n",
    "    y_test_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Calculate multiple metrics\n",
    "    train_metrics = {\n",
    "        'accuracy': accuracy_score(y_train, y_train_pred),\n",
    "        'precision': precision_score(y_train, y_train_pred, average='macro'),\n",
    "        'recall': recall_score(y_train, y_train_pred, average='macro'),\n",
    "        'f1': f1_score(y_train, y_train_pred, average='macro')\n",
    "    }\n",
    "    \n",
    "    test_metrics = {\n",
    "        'accuracy': accuracy_score(y_test, y_test_pred),\n",
    "        'precision': precision_score(y_test, y_test_pred, average='macro'),\n",
    "        'recall': recall_score(y_test, y_test_pred, average='macro'),\n",
    "        'f1': f1_score(y_test, y_test_pred, average='macro')\n",
    "    }\n",
    "    \n",
    "    # Cross validation (using accuracy by default)\n",
    "    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='accuracy')\n",
    "    \n",
    "    # Store performance metrics\n",
    "    model_performances[model_name] = {\n",
    "        'train_metrics': train_metrics,\n",
    "        'test_metrics': test_metrics,\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std()\n",
    "    }\n",
    "    \n",
    "    # Individual model evaluation\n",
    "    train_and_evaluate_model(\n",
    "        model, \n",
    "        X_train_scaled, \n",
    "        X_test_scaled, \n",
    "        y_train, \n",
    "        y_test, \n",
    "        model_name\n",
    "    )"
   ],
   "id": "131766edf77e669f",
   "execution_count": 18,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T17:03:20.299229Z",
     "start_time": "2024-12-21T17:03:20.101002Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create performance comparison visualizations\n",
    "def plot_model_comparison(model_performances):\n",
    "    # Prepare data for plotting\n",
    "    models = list(model_performances.keys())\n",
    "    train_scores = [perf['train_metrics']['accuracy'] for perf in model_performances.values()]\n",
    "    test_scores = [perf['test_metrics']['accuracy'] for perf in model_performances.values()]\n",
    "    cv_means = [perf['cv_mean'] for perf in model_performances.values()]\n",
    "    cv_stds = [perf['cv_std'] for perf in model_performances.values()]\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12))\n",
    "    \n",
    "    # Common x-axis positions for both plots\n",
    "    x = np.arange(len(models))\n",
    "    width = 0.35\n",
    "    \n",
    "    # Bar plot for train vs test scores\n",
    "    ax1.bar(x - width/2, train_scores, width, label='Training Score', color='skyblue')\n",
    "    ax1.bar(x + width/2, test_scores, width, label='Test Score', color='lightcoral')\n",
    "    \n",
    "    ax1.set_ylabel('Accuracy Score')\n",
    "    ax1.set_title('Model Performance Comparison')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(models, rotation=45, ha='right')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Cross-validation scores\n",
    "    ax2.bar(x, cv_means, alpha=0.7, color='lightgreen')\n",
    "    \n",
    "    # Add text annotations for standard deviation\n",
    "    for i, (mean, std) in enumerate(zip(cv_means, cv_stds)):\n",
    "        ax2.text(i, mean, f'±{std*2:.3f}', ha='center', va='bottom')\n",
    "        \n",
    "    ax2.set_ylabel('Cross-validation Score')\n",
    "    ax2.set_title('5-Fold Cross-validation Scores (with ±2σ shown above bars)')\n",
    "    ax2.set_xticks(x)\n",
    "    ax2.set_xticklabels(models, rotation=45, ha='right')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    # Set y-axis to start from 0.5 for better visualization of differences\n",
    "    ax2.set_ylim(0.5, 1.0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary table\n",
    "    print(\"\\nModel Performance Summary:\")\n",
    "    print(\"-\" * 100)\n",
    "    print(f\"{'Model':<20} {'Metric':<10} {'Train':>10} {'Test':>10} {'CV Mean':>10} {'CV Std':>10}\")\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
    "    for model_name, perf in model_performances.items():\n",
    "        for metric in metrics:\n",
    "            print(f\"{model_name if metric=='accuracy' else '':20} \"\n",
    "                  f\"{metric:<10} \"\n",
    "                  f\"{perf['train_metrics'][metric]:>10.3f} \"\n",
    "                  f\"{perf['test_metrics'][metric]:>10.3f} \"\n",
    "                  f\"{perf['cv_mean'] if metric=='accuracy' else '-':>10} \"\n",
    "                  f\"{perf['cv_std'] if metric=='accuracy' else '-':>10}\")\n",
    "\n",
    "# Generate comparison plots and summary\n",
    "plot_model_comparison(model_performances)"
   ],
   "id": "2d27e81428a8e826",
   "execution_count": 20,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "c022a92621986a1a",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
