{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HvCLMpJQle1g",
    "outputId": "a29e10b9-3522-414a-965f-a5d9a68fbb16",
    "ExecuteTime": {
     "end_time": "2024-12-21T16:33:13.433905Z",
     "start_time": "2024-12-21T16:33:12.508154Z"
    }
   },
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"mlg-ulb/creditcardfraud\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(path + \"/creditcard.csv\")\n",
    "df"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "id": "mzYmWwLYl8zZ",
    "outputId": "a6493018-603e-455a-afb9-dbab145f89ed",
    "ExecuteTime": {
     "end_time": "2024-12-21T16:33:14.663916Z",
     "start_time": "2024-12-21T16:33:13.434912Z"
    }
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Prepare data\n",
    "numeric_features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "numeric_features.remove('Class')  # Remove the class column\n",
    "data = df[numeric_features]\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data)  # This is already a numpy array\n",
    "data_scaled"
   ],
   "metadata": {
    "id": "Ay9EC809ntFG",
    "ExecuteTime": {
     "end_time": "2024-12-21T16:33:15.240840Z",
     "start_time": "2024-12-21T16:33:14.664924Z"
    }
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T16:09:39.711795Z",
     "start_time": "2024-12-21T16:05:24.917594Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def optimize_dbscan_params(data_scaled, k=4, sample_size=10**10):\n",
    "    \"\"\"\n",
    "    Optimize DBSCAN parameters for scaled data\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data_scaled : numpy array\n",
    "        Scaled input data\n",
    "    k : int, default=4\n",
    "        Number of neighbors for k-distance\n",
    "    sample_size : int, default=10000\n",
    "        Maximum number of points to use for optimization\n",
    "    \"\"\"\n",
    "    # Convert to numpy array if it's a DataFrame\n",
    "    if isinstance(data_scaled, pd.DataFrame):\n",
    "        data_scaled = data_scaled.to_numpy()\n",
    "    \n",
    "    # Sample data if needed\n",
    "    if len(data_scaled) > sample_size:\n",
    "        indices = np.random.choice(len(data_scaled), sample_size, replace=False)\n",
    "        data_sample = data_scaled[indices]\n",
    "    else:\n",
    "        data_sample = data_scaled\n",
    "\n",
    "    # Calculate k-distances\n",
    "    nbrs = NearestNeighbors(n_neighbors=k+1).fit(data_sample)\n",
    "    distances, _ = nbrs.kneighbors(data_sample)\n",
    "    \n",
    "    # Sort k-distances\n",
    "    k_distances = np.sort(distances[:, -1])\n",
    "    \n",
    "    # Plot k-distance graph\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(len(k_distances)), k_distances)\n",
    "    plt.xlabel('Points sorted by distance')\n",
    "    plt.ylabel(f'{k}-distance')\n",
    "    plt.title(f'K-distance Graph (k={k})')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Find elbow point\n",
    "    elbow_point = find_elbow(k_distances)\n",
    "    plt.axhline(y=elbow_point, color='r', linestyle='--', \n",
    "                label=f'Suggested eps: {elbow_point:.3f}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return elbow_point, k + 1\n",
    "\n",
    "def find_elbow(k_distances):\n",
    "    \"\"\"Find elbow point using maximum curvature method\"\"\"\n",
    "    nPoints = len(k_distances)\n",
    "    allCoord = np.vstack((range(nPoints), k_distances)).T\n",
    "    firstPoint = allCoord[0]\n",
    "    lineVec = allCoord[-1] - allCoord[0]\n",
    "    lineVecN = lineVec / np.sqrt(np.sum(lineVec**2))\n",
    "    \n",
    "    vecFromFirst = allCoord - firstPoint\n",
    "    scalarProduct = np.sum(vecFromFirst * lineVecN, axis=1)\n",
    "    vecFromFirstParallel = np.outer(scalarProduct, lineVecN)\n",
    "    vecToLine = vecFromFirst - vecFromFirstParallel\n",
    "    distToLine = np.sqrt(np.sum(vecToLine ** 2, axis=1))\n",
    "    \n",
    "    idxOfBestPoint = np.argmax(distToLine)\n",
    "    return k_distances[idxOfBestPoint]\n",
    "\n",
    "\n",
    "\n",
    "# Optional: Try different k values\n",
    "k_values = [3, 4, 5, 6]\n",
    "results = []\n",
    "for k in k_values:\n",
    "    print(f\"\\nTrying k={k}:\")\n",
    "    eps, min_samples = optimize_dbscan_params(data_scaled, k=k)\n",
    "    results.append({\n",
    "        'k': k,\n",
    "        'eps': eps,\n",
    "        'min_samples': min_samples\n",
    "    })\n",
    "    print(f\"eps: {eps:.3f}\")\n",
    "    print(f\"min_samples: {min_samples}\")\n",
    "\n",
    "# Create a summary DataFrame of results\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nSummary of all trials:\")\n",
    "print(results_df)"
   ],
   "execution_count": 7,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T16:38:52.976362Z",
     "start_time": "2024-12-21T16:36:43.719269Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm \n",
    "\n",
    "def balanced_sampling_for_dbscan(data_scaled, true_labels, sample_size=50000, fraud_ratio=0.1):\n",
    "    \"\"\"\n",
    "    Sample data with increased representation of fraud cases\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data_scaled : array-like\n",
    "        Scaled input data\n",
    "    true_labels : array-like\n",
    "        Binary fraud labels (0: normal, 1: fraud)\n",
    "    sample_size : int\n",
    "        Desired total sample size\n",
    "    fraud_ratio : float\n",
    "        Desired ratio of fraud cases in sample (e.g., 0.3 = 30% frauds)\n",
    "    \"\"\"\n",
    "    # Separate fraud and normal cases\n",
    "    fraud_mask = true_labels == 1\n",
    "    normal_mask = ~fraud_mask\n",
    "    \n",
    "    fraud_data = data_scaled[fraud_mask]\n",
    "    normal_data = data_scaled[normal_mask]\n",
    "    \n",
    "    # Calculate sample sizes\n",
    "    n_frauds = int(sample_size * fraud_ratio)\n",
    "    n_normal = sample_size - n_frauds\n",
    "    \n",
    "    # Ensure we don't sample more frauds than available\n",
    "    n_frauds = min(n_frauds, len(fraud_data))\n",
    "    \n",
    "    # Sample from each class\n",
    "    np.random.seed(42)  # for reproducibility\n",
    "    \n",
    "    # Sample with replacement for fraud (if needed)\n",
    "    if n_frauds > len(fraud_data):\n",
    "        fraud_indices = np.random.choice(len(fraud_data), n_frauds, replace=True)\n",
    "    else:\n",
    "        fraud_indices = np.random.choice(len(fraud_data), n_frauds, replace=False)\n",
    "        \n",
    "    normal_indices = np.random.choice(len(normal_data), n_normal, replace=False)\n",
    "    \n",
    "    # Combine samples\n",
    "    sampled_data = np.vstack([\n",
    "        fraud_data[fraud_indices],\n",
    "        normal_data[normal_indices]\n",
    "    ])\n",
    "    \n",
    "    sampled_labels = np.hstack([\n",
    "        np.ones(n_frauds),\n",
    "        np.zeros(n_normal)\n",
    "    ])\n",
    "    \n",
    "    # Shuffle the combined data\n",
    "    shuffle_idx = np.random.permutation(len(sampled_data))\n",
    "    \n",
    "    print(f\"Original fraud ratio: {fraud_mask.mean():.4f}\")\n",
    "    print(f\"Sampled fraud ratio: {(sampled_labels == 1).mean():.4f}\")\n",
    "    print(f\"Total sample size: {len(sampled_data)}\")\n",
    "    \n",
    "    return sampled_data[shuffle_idx], sampled_labels[shuffle_idx]\n",
    "\n",
    "# Modified evaluation function\n",
    "def evaluate_dbscan_params(data_scaled, param_pairs, true_labels, sample_size=50000, fraud_ratio=0.3):\n",
    "    \"\"\"Modified evaluation function with balanced sampling\"\"\"\n",
    "    # Sample data with increased fraud representation\n",
    "    data_sample, labels_sample = balanced_sampling_for_dbscan(\n",
    "        data_scaled, \n",
    "        true_labels, \n",
    "        sample_size=sample_size,\n",
    "        fraud_ratio=fraud_ratio\n",
    "    )\n",
    "    \n",
    "    results = []\n",
    "    for eps, minpts in tqdm(param_pairs, desc=\"Testing parameter pairs\"):\n",
    "        dbscan = DBSCAN(\n",
    "            eps=eps, \n",
    "            min_samples=minpts, \n",
    "            n_jobs=-1,\n",
    "            algorithm='kd_tree'\n",
    "        )\n",
    "        clusters = dbscan.fit_predict(data_sample)\n",
    "        \n",
    "        # Skip if all points are noise\n",
    "        if len(np.unique(clusters)) <= 1:\n",
    "            tqdm.write(\"Skipping: All points classified as noise\")\n",
    "            continue\n",
    "            \n",
    "        # Calculate metrics\n",
    "        n_clusters = len(set(clusters)) - (1 if -1 in clusters else 0)\n",
    "        n_noise = list(clusters).count(-1)\n",
    "        \n",
    "        # Calculate fraud concentration in each cluster\n",
    "        cluster_metrics = []\n",
    "        for cluster_id in set(clusters):\n",
    "            if cluster_id == -1:  # Skip noise points\n",
    "                continue\n",
    "                \n",
    "            cluster_mask = clusters == cluster_id\n",
    "            cluster_frauds = labels_sample[cluster_mask].sum()\n",
    "            cluster_total = cluster_mask.sum()\n",
    "            fraud_ratio = cluster_frauds / cluster_total\n",
    "            \n",
    "            cluster_metrics.append({\n",
    "                'cluster_id': cluster_id,\n",
    "                'size': cluster_total,\n",
    "                'n_frauds': cluster_frauds,\n",
    "                'fraud_ratio': fraud_ratio\n",
    "            })\n",
    "        \n",
    "        # Find cluster with highest fraud concentration\n",
    "        if cluster_metrics:\n",
    "            best_cluster = max(cluster_metrics, key=lambda x: x['fraud_ratio'])\n",
    "            tqdm.write(f\"Found {n_clusters} clusters, best fraud ratio: {best_cluster['fraud_ratio']:.3f}\")\n",
    "        else:\n",
    "            best_cluster = {'fraud_ratio': 0, 'size': 0, 'n_frauds': 0}\n",
    "            tqdm.write(\"No valid clusters found\")\n",
    "        \n",
    "        results.append({\n",
    "            'eps': eps,\n",
    "            'min_samples': minpts,\n",
    "            'n_clusters': n_clusters,\n",
    "            'n_noise': n_noise,\n",
    "            'noise_ratio': n_noise / len(data_sample),\n",
    "            'best_cluster_fraud_ratio': best_cluster['fraud_ratio'],\n",
    "            'best_cluster_size': best_cluster['size'],\n",
    "            'best_cluster_frauds': best_cluster['n_frauds']\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "# Define the specific eps-minpts pairs\n",
    "param_pairs = [\n",
    "    (5.66, 4),  # k=3 result\n",
    "    (5.73, 5),  # k=4 result\n",
    "    (5.79, 6),  # k=5 result\n",
    "    (5.91, 7)   # k=6 result\n",
    "]\n",
    "\n",
    "# Evaluate parameters\n",
    "results_df = evaluate_dbscan_params(\n",
    "    data_scaled,\n",
    "    param_pairs,\n",
    "    df['Class'].values , # true fraud labels,\n",
    "    sample_size=50000,\n",
    "    fraud_ratio=0.3\n",
    ")\n",
    "\n",
    "# Sort by fraud concentration in best cluster\n",
    "print(\"\\nResults sorted by fraud concentration:\")\n",
    "print(results_df.sort_values('best_cluster_fraud_ratio', ascending=False))\n",
    "\n",
    "# Plot some key metrics\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.scatter(results_df['eps'], results_df['n_clusters'], \n",
    "            c=results_df['best_cluster_fraud_ratio'], cmap='viridis')\n",
    "plt.colorbar(label='Fraud Ratio in Best Cluster')\n",
    "plt.xlabel('eps')\n",
    "plt.ylabel('Number of Clusters')\n",
    "plt.title('Number of Clusters vs eps')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.scatter(results_df['eps'], results_df['noise_ratio'],\n",
    "            c=results_df['best_cluster_fraud_ratio'], cmap='viridis')\n",
    "plt.colorbar(label='Fraud Ratio in Best Cluster')\n",
    "plt.xlabel('eps')\n",
    "plt.ylabel('Noise Ratio')\n",
    "plt.title('Noise Ratio vs eps')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "execution_count": 7,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T16:48:49.141856Z",
     "start_time": "2024-12-21T16:48:04.697578Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import seaborn as sns\n",
    "\n",
    "def analyze_dbscan_clusters(data_scaled, clusters, true_labels, feature_names):\n",
    "    \"\"\"\n",
    "    Analyze DBSCAN clustering results\n",
    "    \"\"\"\n",
    "    # Convert to DataFrame for easier analysis\n",
    "    df_analysis = pd.DataFrame(data_scaled, columns=feature_names)\n",
    "    df_analysis['Cluster'] = clusters\n",
    "    df_analysis['Is_Fraud'] = true_labels\n",
    "    \n",
    "    # 1. Basic cluster statistics\n",
    "    print(\"\\n=== Basic Cluster Statistics ===\")\n",
    "    cluster_stats = df_analysis.groupby('Cluster').agg({\n",
    "        'Is_Fraud': ['count', 'sum', 'mean']\n",
    "    }).round(4)\n",
    "    cluster_stats.columns = ['Size', 'Frauds', 'Fraud_Ratio']\n",
    "    print(\"\\nCluster sizes and fraud ratios:\")\n",
    "    print(cluster_stats)\n",
    "    \n",
    "    # 2. Feature characteristics per cluster\n",
    "    print(\"\\n=== Feature Characteristics by Cluster ===\")\n",
    "    cluster_features = df_analysis.groupby('Cluster')[feature_names].agg(['mean', 'std'])\n",
    "    \n",
    "    # Find top distinguishing features for each cluster\n",
    "    print(\"\\nTop distinguishing features per cluster:\")\n",
    "    for cluster in sorted(set(clusters)):\n",
    "        if cluster == -1:\n",
    "            continue\n",
    "        \n",
    "        # Calculate z-scores for this cluster vs others\n",
    "        cluster_mask = clusters == cluster\n",
    "        cluster_data = data_scaled[cluster_mask]\n",
    "        other_data = data_scaled[~cluster_mask]\n",
    "        \n",
    "        # Calculate effect sizes (Cohen's d)\n",
    "        effect_sizes = []\n",
    "        for i, feature in enumerate(feature_names):\n",
    "            c_mean = np.mean(cluster_data[:, i])\n",
    "            c_std = np.std(cluster_data[:, i])\n",
    "            o_mean = np.mean(other_data[:, i])\n",
    "            o_std = np.std(other_data[:, i])\n",
    "            \n",
    "            # Pooled standard deviation\n",
    "            pooled_std = np.sqrt(((c_std ** 2) + (o_std ** 2)) / 2)\n",
    "            effect_size = abs(c_mean - o_mean) / pooled_std\n",
    "            effect_sizes.append((feature, effect_size))\n",
    "        \n",
    "        # Sort and display top features\n",
    "        top_features = sorted(effect_sizes, key=lambda x: x[1], reverse=True)[:5]\n",
    "        print(f\"\\nCluster {cluster} (Size: {sum(cluster_mask)}):\")\n",
    "        for feature, effect in top_features:\n",
    "            print(f\"{feature}: effect size = {effect:.3f}\")\n",
    "    \n",
    "    # 3. Fraud concentration analysis\n",
    "    print(\"\\n=== Fraud Analysis ===\")\n",
    "    fraud_mask = true_labels == 1\n",
    "    print(f\"Total frauds captured in clusters: {sum(fraud_mask & (clusters != -1))}\")\n",
    "    print(f\"Total frauds in noise: {sum(fraud_mask & (clusters == -1))}\")\n",
    "    \n",
    "    # Calculate fraud capture rate per cluster\n",
    "    total_frauds = sum(fraud_mask)\n",
    "    for cluster in sorted(set(clusters)):\n",
    "        if cluster == -1:\n",
    "            continue\n",
    "        cluster_frauds = sum(fraud_mask & (clusters == cluster))\n",
    "        print(f\"Cluster {cluster} captures {cluster_frauds} ({cluster_frauds/total_frauds*100:.1f}%) of all frauds\")\n",
    "    \n",
    "    # 4. Visualization of key patterns\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Plot 1: Cluster sizes\n",
    "    plt.subplot(131)\n",
    "    cluster_sizes = df_analysis['Cluster'].value_counts()\n",
    "    plt.bar(cluster_sizes.index.astype(str), cluster_sizes.values)\n",
    "    plt.title('Cluster Sizes')\n",
    "    plt.xlabel('Cluster')\n",
    "    plt.ylabel('Number of Points')\n",
    "    \n",
    "    # Plot 2: Fraud ratios\n",
    "    plt.subplot(132)\n",
    "    fraud_ratios = df_analysis.groupby('Cluster')['Is_Fraud'].mean()\n",
    "    plt.bar(fraud_ratios.index.astype(str), fraud_ratios.values)\n",
    "    plt.title('Fraud Ratios by Cluster')\n",
    "    plt.xlabel('Cluster')\n",
    "    plt.ylabel('Fraud Ratio')\n",
    "    \n",
    "    # Plot 3: Feature importance heatmap\n",
    "    plt.subplot(133)\n",
    "    cluster_means = df_analysis.groupby('Cluster')[feature_names].mean()\n",
    "    sns.heatmap(cluster_means, cmap='RdBu_r', center=0)\n",
    "    plt.title('Feature Patterns by Cluster')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return cluster_stats, cluster_features\n",
    "\n",
    "# Get a sample for detailed analysis\n",
    "data_sample, labels_sample = balanced_sampling_for_dbscan(\n",
    "    data_scaled, \n",
    "    df['Class'].values,\n",
    "    sample_size=50000,\n",
    "    fraud_ratio=0.00172\n",
    ")\n",
    "\n",
    "# Run DBSCAN with best parameters\n",
    "best_dbscan = DBSCAN(\n",
    "    eps=5.66,\n",
    "    min_samples=4,\n",
    "    n_jobs=-1,\n",
    "    algorithm='kd_tree'\n",
    ")\n",
    "best_clusters = best_dbscan.fit_predict(data_sample)\n",
    "\n",
    "# Define feature names\n",
    "feature_names = [f'V{i}' for i in range(1, 29)] + ['Time', 'Amount']\n",
    "\n",
    "# Analyze clusters\n",
    "cluster_stats, cluster_features = analyze_dbscan_clusters(\n",
    "    data_sample,\n",
    "    best_clusters,\n",
    "    labels_sample,\n",
    "    feature_names\n",
    ")"
   ],
   "execution_count": 9,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "outputs": []
  }
 ]
}
